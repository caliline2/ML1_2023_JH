library(glmnet)  # load the glmnet package for lasso regression
library(tidyverse)
library(caret)


options(scipen = 99)  # avoid scientific notation
options(max.print = 20)  # avoid long outputs
options(str = strOptions(list.len = 20))  # avoid long outputs
theme_set(theme_minimal())

# Defining MSE function

```{r model}
f_y_x <- function(x1) {
    x1^2 - 1.5 * x1
}
```


n <- 20  # set the sample size

set.seed(123)  # set a random seed for reproducibility

data <- tibble(
    x1 = runif(n),
    x2 = runif(n),
    y = f_y_x(x1) + rnorm(n)
)

X1 <- runif(n, min = 0, max = 1)  # generate uniform[0,1] random variables
X2 <- runif(n, min = 0, max = 1)
eps <- rnorm(n, mean = 0, sd = 2)  # generate normal errors with mean 0 and sd 2
Y <- 1*X1 + 1*X2 + eps  # generate the response variable using the model



# create a matrix of the predictors and add a column of ones for the intercept
X <- cbind(rep(1, n), X1, X2)

# create a grid of lambda values to use in the lasso regression
lambda <- seq(0, 2, length = 100)

# estimate the lasso regression for each value of lambda in the grid
lasso.fit <- glmnet(X, Y, alpha = 1, lambda = lambda)

# plot the coefficient paths for each predictor
plot(lasso.fit, xvar = "lambda", label = TRUE)

# set the fixed value of x
x <- c(1, 0.5, 0.5)

# compute the lasso regression coefficients for each value of lambda
beta.lasso <- predict(lasso.fit, newx = matrix(x, ncol = 3))

# compute the prediction for each value of lambda
f.lasso <- beta.lasso[1] + beta.lasso[2]*x[2] + beta.lasso[3]*x[3]

# compute the true prediction for x = (0.5, 0.5)
f.true <- 0 + 1*0.5 + 1*0.5

# compute the bias, variance, and MSE for each value of lambda
bias2 <- (


# Load the data
data <- read.csv("PCA_data.csv", header = TRUE)

# Check the dimensions of the data
dim(data) # should return (1000, 54)

# Subset the data into training and test sets
train <- data[1:500, ]
test <- data[501:1000, ]


# Extract the X variables
X_train <- train[, 1:50]
X_test <- test[, 1:50]

# Compute the principal components
pca <- prcomp(X_train, center = TRUE, scale. = TRUE)

# Extract the first 10 principal component vectors
pcs <- pca$rotation[, 1:10]

# Compute the principal component scores for both training and test sets
Z_train <- as.matrix(X_train) %*% pcs
Z_test <- as.matrix(X_test) %*% pcs

# Extract the Y variable
Y_train <- train[, "Y"]
Y_test <- test[, "Y"]

# OLS regression of Y on X1,...,X50 over the training sample
ols_X <- lm(Y_train ~ X_train)

# OLS regression of Y on Z1,...,Zk over the training sample for k = 1, 5, 10
ols_Z1 <- lm(Y_train ~ Z_train[, 1])
ols_Z5 <- lm(Y_train ~ Z_train[, 1:5])
ols_Z10 <- lm(Y_train ~ Z_train)

# Predict the outcomes for the test set using each of the four models
Yhat_X <- predict(ols_X, newdata = test)
Yhat_Z1 <- predict(ols_Z1, newdata = data.frame(Z_test[, 1]))
Yhat_Z5 <- predict(ols_Z5, newdata = data.frame(Z_test[, 1:5]))
Yhat_Z10 <- predict(ols_Z10, newdata = data.frame(Z_test))

# Compute the mean squared prediction error for each model
mspe_X <- mean((Y_test - Yhat_X)^2)
mspe_Z1 <- mean((Y_test - Yhat_Z1)^2)
mspe_Z5 <- mean((Y_test - Yhat_Z5)^2)
mspe_Z10 <- mean((Y_test - Yhat_Z10)^2)

# Print the results
print(paste("MSPE (X):", round(mspe_X, 4)))
print(paste("MSPE (Z1):", round(mspe_Z1, 4)))
print(paste("MSPE (Z5):", round(mspe_Z5, 4)))
print(paste("MSPE (Z10):", round(mspe_Z10, 4)))

################################################################
# Lab 2


```{r estimation}
estimated_models <- list(
    true = lm(y ~ x1 + I(x1^2), data),
    simple = lm(y ~ x1, data),
    full = lm(y ~ polym(x1, x2, degree = 2, raw = TRUE), data)
)
```

```{r evaluator-by-MSE}
# evaluate on multiple observations of a single dataset
calculateMSE <- function(prediction, y_exp) {
    mean((prediction - y_exp)^2)
}
```

```{r evaluation}
map(estimated_models, ~calculateMSE(predict(.x), f_y_x(data$x1)))
```

socrative.com
Student login: enter room “CEUML”

# Recap: caret

```{r estimate-lm-with-caret}
estimated_models_caret <- list(
    simple = train(
        y ~ x1, data, 
        method = "lm",
        trControl = trainControl(method = "none")
    ),
    full = train(
        y ~ polym(x1, x2, degree = 2, raw = TRUE), data, 
        method = "lm",
        trControl = trainControl(method = "none")
    )
)
```

```{r evaluation-caret}
map(estimated_models_caret, ~calculateMSE(predict(.x), f_y_x(data$x1)))
```

Let's create the _feature matrix_ beforehand (without the intercept as it will be included automatically in the model building phase).

```{r model-matrix}
expanded_data <- model.matrix(y ~ polym(x1, x2, degree = 2, raw = TRUE) + 0, data)
head(expanded_data)
```

!!!DANGER ALERT!!! Some exotic regex magic follows...
The functions below transform the names of our generated variables from `polym()` to some easier-to-digest alternatives.
No effect at all on what is calculated.

```{r tidy-var-names-functions}
tidyPolyVarNames <- function(var_string) {
    # handy tool for regexing in R: https://spannbaueradam.shinyapps.io/r_regex_tester/
    poly_str_match <- str_match(var_string, "polym\\((?<vars>.*), degree.*\\)(?<degrees>[0-9.]+)")
    vars <- str_extract_all(poly_str_match[, "vars"], "[A-z0-9]+")[[1]]
    degrees <- str_extract_all(poly_str_match[, "degrees"], "[0-9]")[[1]]
    map2(vars, degrees, ~powerToVarName(.x, .y)) |>
        compact() |>
        paste(collapse = "*")
}
powerToVarName <- function(var_name, power) {
    if (power == "0") {
        NULL
    } else if (power == "1") {
        var_name
    } else {
        paste(var_name, power, sep = "^")
    }
}
```

```{r tidy-var-names}
tidy_colnames <- map_chr(colnames(expanded_data), tidyPolyVarNames)
colnames(expanded_data) <- tidy_colnames
head(expanded_data)
```

```{r working-evaluation-caret}
estimated_models_caret <- list(
    simple = train(
        y ~ x1, data,
        method = "lm",
        trControl = trainControl(method = "none")
    ),
    full = train(
        x = expanded_data, y = data$y,
        method = "lm",
        trControl = trainControl(method = "none")
    )
)
map(estimated_models_caret, ~calculateMSE(predict(.x), f_y_x(data$x1)))
```

```{r compare-coefs}
coef(estimated_models$full)
coef(estimated_models_caret$full$finalModel)
```

# Brilliant idea: regularization

How do we know if we should use the simpler model? 
Which variables to use in the simpler model?
Solution: penalize complex models -> let the data choose the variables that matter (LASSO)



```{r lasso-with-no-penalty-is-lm}
full_glmnet <- train(
    x = expanded_data, y = data$y,
    method = "glmnet",
    trControl = trainControl(method = "none"),
    tuneGrid = expand.grid(alpha = 1, lambda = 0)
)
coef(full_glmnet$finalModel, 0)  # for some strange reason we have to add lambda = 0 again
coef(estimated_models_caret$full$finalModel)  # different optimization algorithms
```

```{r compare-mse-lm-lasso}
calculateMSE(predict(full_glmnet), f_y_x(data$x1))
calculateMSE(predict(estimated_models_caret$full), f_y_x(data$x1))
```




```{r first-lasso}
first_lasso <- train(
    x = expanded_data, y = data$y,
    method = "glmnet",
    trControl = trainControl(method = "none"),
    tuneGrid = expand.grid(alpha = 1, lambda = 0.1)
)
calculateMSE(predict(first_lasso), f_y_x(data$x1))
```


# Hyperparameter-tuning

```{r manual-hyperparam-tuning}
lambda_values <- seq(0, 0.5, 0.01)
results <- map_df(lambda_values, ~{
    model <- train(
        x = expanded_data, y = data$y,
        method = "glmnet",
        trControl = trainControl(method = "none"),
        tuneGrid = expand.grid(alpha = 1, lambda = .x)
    )
    mse <- calculateMSE(predict(model), f_y_x(data$x1))    
    tibble(lambda = .x, MSE = mse)
})
ggplot(results, aes(lambda, MSE)) + geom_line(linewidth = 1)
```

```{r auto-hyperparam-tuning}
caret_lasso <- train(
    x = expanded_data, y = data$y,
    method = "glmnet",
    tuneGrid = expand.grid(alpha = 1, lambda = lambda_values)
)
plot(caret_lasso)
plot(caret_lasso$finalModel, xvar = "lambda")
```


# Simulation

```{r}
f2_y_x <- function(x1, x2) {
    x1^2 - 1.5 * x2
}
```

```{r simulation-function}
runSimulationStep <- function(n = 100, sd_e = 1, lambda = 0.01) {
    # Step 1: Simulate the data
    data <- tibble(
        x1 = runif(n),
        x2 = runif(n),
        y = f2_y_x(x1, x2) + rnorm(n, sd = sd_e)
    )
    
    expanded_data <- model.matrix(y ~ polym(x1, x2, degree = 2, raw = TRUE) + 0, data)
    tidy_colnames <- map_chr(colnames(expanded_data), tidyPolyVarNames)
    colnames(expanded_data) <- tidy_colnames
    
    # Step 2: Estimation
    estimated_models <- list(
        simple = train(
            y ~ x1, data,
            method = "lm",
            trControl = trainControl(method = "none")
        ),
        full = train(
            x = expanded_data, y = data$y,
            method = "lm",
            trControl = trainControl(method = "none")
        ),
        lasso = train(
            x = expanded_data, y = data$y,
            method = "glmnet",
            trControl = trainControl(method = "none"),
            tuneGrid = expand.grid(alpha = 1, lambda = lambda)
        )
    )
    
    lasso_coefs <- coef(estimated_models$lasso$finalModel, lambda)[, 1]
    
    # Step 3: Evaluation
    list(
        mse = map(estimated_models, ~calculateMSE(predict(.x), f2_y_x(data$x1, data$x2))),
        lasso_nonzero = lasso_coefs[lasso_coefs != 0]
    )
    
}
```

```{r sample-simulation}
runSimulationStep()
```

```{r mc-simulation}
set.seed(20230213)
n_sim <- 1000
simulated_results <- map(seq(n_sim), ~runSimulationStep())
```

```{r mse-table}
pivot_longer(map_df(simulated_results, "mse"), cols = everything()) |>
    group_by(name) |>
    summarise(
        avg = mean(value),
        median = median(value),
        q95 = quantile(value, 0.95)
    )
```

```{r how-many-coefs-are-nonzero}
nonzero_coefs <- map(simulated_results, "lasso_nonzero") |>
    map(tail, -1)
map_dbl(nonzero_coefs, length) |>
    table()
```

```{r which-coefs-are-nonzero}
map(nonzero_coefs, names) |>
    unlist() |>
    table()
```







#######################################################################3
# Lab 3


```{r calculate-mse}
calculateMSE <- function(prediction, y_exp) {
    mean((prediction - y_exp)^2)
}
```

```{r feature-matrix}
# called extended_data in lab2
feature_matrix <- model.matrix(y ~ polym(x1, x2, degree = 2, raw = TRUE) + 0, data)
```

```{r caret}
caret_lasso <- train(
    x = feature_matrix, y = data$y,
    method = "glmnet",
    trControl = trainControl(method = "none"),
    tuneGrid = expand.grid(alpha = 1, lambda = 0.1)
)
calculateMSE(predict(caret_lasso), f_y_x(data$x1))
```

```{r simple-glmnet}
base_lasso <- glmnet(
    x = feature_matrix, y = data$y,
    alpha = 1, lambda = 0.1
)
# caret predicts on the original data by default -- glmnet needs an explicit `newx` parameter
calculateMSE(predict(base_lasso, feature_matrix), f_y_x(data$x1))
```

```{r simple-glmnet-with-multiple-lambdas}
lambda_values <- seq(0, 0.2, 0.01)
lasso_models <- glmnet(
    x = feature_matrix, y = data$y,
    alpha = 1, lambda = lambda_values
)
lasso_models
```
```{r predict-with-glmnet}
# we get a list of prediction for each lambda value
predict(lasso_models, feature_matrix)
```
```{r coefficients-from-glmnet}
lasso_models$beta[, 15:20]
```


```{r number-of-nonzero-coefficients}
lasso_models$df
```


```{r predict-on-a-given-point}
point_to_evaluate <- c(0, 0, 0, 0, 0)  # rep(0, 5)
predict(lasso_models, point_to_evaluate)
```

socrative.com

# Bias-variance simulation by lambda

```{r simulation-function}
runSimulationStep <- function(n = 100, sd_e = 1, lambda_values = seq(0, 0.4, 0.01)) {
    # Step 1: Simulate the data
    data <- tibble(
        x1 = runif(n),
        x2 = runif(n),
        y = f_y_x(x1) + rnorm(n, sd = sd_e)
    )
    # Step 2: Estimation
    feature_matrix <- model.matrix(y ~ polym(x1, x2, degree = 2, raw = TRUE) + 0, data)
    lasso_models <- glmnet(
        x = feature_matrix, y = data$y,
        alpha = 1, lambda = lambda_values
    )
    # Step 3: Prediction
    tibble(
        lambda = round(lasso_models$lambda, digits = 2),  # rounding is needed to ensure that the lambda values remain the same across runs (glm implementation is buggy)
        predictions = as.numeric(predict(lasso_models, point_to_evaluate)),
        n_nz_coefs = lasso_models$df
    )
}
```

```{r sample-simulation}
runSimulationStep()
```

```{r mc-simulation}
set.seed(20230220)
n_sim <- 1000
simulated_results <- map_df(seq(n_sim), ~runSimulationStep(sd_e = 1))
```

```{r visualize-bias-variance}
group_by(simulated_results, lambda) |>
    summarise(
        bias2 = (mean(predictions) - f_y_x(0))^2,
        var = var(predictions),
        MSE = bias2 + var
    ) |>
    pivot_longer(cols = bias2:MSE) |>
    ggplot(aes(lambda, value, col = name)) +
    geom_line(linewidth = 1)
```

```{r number-of-nonzero-coefficients-by-lambda}
group_by(simulated_results, lambda) |>
    summarise(n_nz_coefs = mean(n_nz_coefs)) |>
    ggplot(aes(lambda, n_nz_coefs)) + 
        geom_line(linewidth = 1)
```


## Principal Component Analysis (PCA)

```{r}
library(factoextra)
```


We transform the coordinates of the original variables to capture as much variation as we can with independent (orthogonal) dimensions.
For a very nice illustration and discussion, see [here](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues).

```{r}
data(decathlon2)
decathlon2
```


## An example with two variables

Take two variables first only. We seek a linear combination of them that
* has the highest possible variance
* with weights that are normalized (their sum of squares are 1)

```{r}
short_runs <- select(decathlon2, X100m, X400m)
ggplot(decathlon2, aes(X100m, X400m)) + geom_point()
```

```{r}
# the variables have totally different variances
summarise(short_runs, across(everything(), var))
```

Demean the data only for easier visualizations.
```{r}
short_runs_demeaned <- mutate(short_runs, across(everything(), ~ .x - mean(.x)))
```

```{r}
ggplot(short_runs_demeaned, aes(X100m, X400m)) +
    geom_point() +
    coord_fixed()
```

The goal is to find a linear combination of the two variables that captures most of the joint variance. Indeed, we see that we get back the weight we obtained from the `prcomp` function.

```{r}
# constraint: w_100m^2 + w_400m^2 = 1
# this means that w_100m = sqrt(1 - w_400m^2)
objective <- function(w_100m) {
    # we want to maximize variance
    # minus: since "optim" applies minimization.
    -var(
        w_100m * short_runs_demeaned$X100m +
        sqrt(1 - w_100m^2) * short_runs_demeaned$X400m
    )
}
optim_result <- optimize(f = objective, interval = c(0, 1), tol = 1e-15)
w_100m <- optim_result$minimum
w_400m <- sqrt(1 - w_100m^2)
message(glue::glue("Weight of 100m: {round(w_100m, 7)} \n Weight of 400m: {round(w_400m, 7)}"))
```

With PCA we can arrive at the same result.

```{r}
# Note from the help of prcomp:
# "The signs of the columns of the rotation matrix are arbitrary, and so may differ between different programs for PCA, and even between different builds of R."
pca_short_run <- prcomp(short_runs_demeaned)
pca_short_run
```

```{r}
pc1 <- pca_short_run$rotation[, "PC1"]
pc1
```

Let us depict this variance-maximizing linear combination of the two variables
in the space of the original variables.
```{r}
ggplot(short_runs_demeaned, aes(X100m, X400m)) +
    geom_point() +
    coord_fixed() +
    geom_abline(slope = pc1[["X400m"]] / pc1[["X100m"]], color = "red")
```

WARNING: this line is very different from regressing X100m on X400m! PCA's aim is to find a line to which if
observations are projected, variance is the highest. Regression: squared errors to be minimized.

```{r}
ggplot(short_runs_demeaned, aes(X100m, X400m)) +
    geom_point() +
    coord_fixed() +
    geom_abline(slope = pc1[["X400m"]] / pc1[["X100m"]], color = "red") +
    geom_abline(slope = coef(lm(X400m ~ X100m, data = short_runs_demeaned))[["X100m"]], color = "blue")
```

See more about it [here](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues/140579#140579).

## The full example

Scaling: necessary to have comparable units of different variables.
(Multiplying a variable by 100 increases the variance and PCA would try to capture that, even though it is only an artifact of units. We don't want that.)
In the previous example we did not scale all variables for the sake of nicer illustrations and we indeed saw that the variable with the higher absolute variance got a lot more weight than the other. This is not really what we are after.

Let's perform PCA on all the variables. The first principal component shows weights for a linear combination of the original variables that maximizes variance (up to constraining weights to have a sum of square equal to 1).
```{r}
results10 <- select(decathlon2, 1:10)
pca_result <- prcomp(results10, scale. = TRUE)
summary(pca_result)
```



Let's check if our numbers are corresponding to our expectations:

- Weights sum to 1

```{r}
pca_result$rotation
colSums(pca_result$rotation^2)
```

- PC-s are orthogonal to each other: they contain "independent" variance from the data (we take the first and the last principal component as an illustration - the orthogonality condition should hold for each possible pairs)

```{r}
sum(pca_result$rotation[, 1] * pca_result$rotation[, 10])
```

- if we take the linear combination of (scaled) original variables with the weights specified by PC1, we get back the standard deviation of PC1

```{r}
pc1_loadings <- pca_result$rotation[, "PC1"]
pc1_value_for_observations <- scale(results10) %*% pc1_loadings  # %*%: matrix-vector product
sd(pc1_value_for_observations)
```

- the total variance equals to the number of variables (due to the scaling)

```{r}
sum(summary(pca_result)$importance[1,]^2)
```

Let's examine our PCA result using the `factoextra::fviz_pca()` function that plots observations as well as original features in the space spanned
by the first two principal components.

```{r}
fviz_pca(pca_result)
```

## PCA on gene data

From the ISLR website, we can download a gene expression data set (Ch10Ex11.csv) that consists of 40 tissue samples with measurements on 1,000 genes. The first 20 samples are from healthy patients, while the second 20 are from a diseased group.

```{r}
genes <- read_csv("https://www.statlearning.com/s/Ch10Ex11.csv", col_names = FALSE) %>%
    t() %>% as_tibble()
dim(genes)
health_status <- c(rep("healthy", 20), rep("diseased", 20))
```


```{r}
# Run PCA on genes data and look at the first ten principal components
genes_pca <- prcomp(genes, rank. = 10)
summary(genes_pca)
```

Let's visualize the first two principal components. You can extract the principal component scores of your features by using our old friend, the `predict()` function.

```{r}
predict(genes_pca) |>
    as_tibble() |>
    mutate(health_status = health_status) |>
    ggplot(aes(PC1, PC2, col = health_status)) +
        geom_vline(xintercept = 0, linetype = "dashed") +
        geom_hline(yintercept = 0, linetype = "dashed") +
        geom_point(size = 3, alpha = 0.6)
```

The first principal component contains all the relevant information we need here. Instead of having to look at 1,000 features it is enough to look at this simple chart.


